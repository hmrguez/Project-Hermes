{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import imghdr\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "image_exts = ['jpeg','png','bmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_class in os.listdir(data_dir):\n",
    "    for image in os.listdir(os.path.join(data_dir,image_class)):\n",
    "        image_path = os.path.join(data_dir,image_class,image)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in image_exts:\n",
    "                os.remove(image_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print('Issue with image {}'.format(image_path))\n",
    "            # os.remove(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data = tf.keras.utils.image_dataset_from_directory(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x,y: (x/255,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data)*.7)\n",
    "val_size = int(len(data)*.2) + 1\n",
    "test_size = int(len(data)*.1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = data.skip(train_size+val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), 1, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(16, (3,3), 1, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', loss=tf.losses.BinaryCrossentropy(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 8s 893ms/step - loss: 1.2533 - accuracy: 0.6172 - val_loss: 0.6309 - val_accuracy: 0.7292\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 8s 890ms/step - loss: 0.6183 - accuracy: 0.7031 - val_loss: 0.6029 - val_accuracy: 0.7188\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 8s 891ms/step - loss: 0.6113 - accuracy: 0.6992 - val_loss: 0.6028 - val_accuracy: 0.7604\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 8s 887ms/step - loss: 0.5548 - accuracy: 0.7227 - val_loss: 0.4800 - val_accuracy: 0.8229\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 8s 889ms/step - loss: 0.4764 - accuracy: 0.8047 - val_loss: 0.3306 - val_accuracy: 0.8125\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 8s 887ms/step - loss: 0.3963 - accuracy: 0.8320 - val_loss: 0.3313 - val_accuracy: 0.8229\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 8s 909ms/step - loss: 0.2798 - accuracy: 0.8828 - val_loss: 0.2511 - val_accuracy: 0.8750\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 8s 939ms/step - loss: 0.2357 - accuracy: 0.9062 - val_loss: 0.1854 - val_accuracy: 0.9375\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 8s 922ms/step - loss: 0.1792 - accuracy: 0.9102 - val_loss: 0.2110 - val_accuracy: 0.9062\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 8s 948ms/step - loss: 0.1643 - accuracy: 0.9375 - val_loss: 0.1578 - val_accuracy: 0.9167\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 8s 986ms/step - loss: 0.1128 - accuracy: 0.9609 - val_loss: 0.0581 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 8s 908ms/step - loss: 0.0608 - accuracy: 0.9883 - val_loss: 0.0514 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 8s 898ms/step - loss: 0.0613 - accuracy: 0.9844 - val_loss: 0.0563 - val_accuracy: 0.9792\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 8s 936ms/step - loss: 0.0385 - accuracy: 0.9961 - val_loss: 0.0210 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 8s 909ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 8s 946ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 8s 939ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 8s 973ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 8s 938ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 8s 946ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train,epochs=20,validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.09964511]], dtype=float32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1test = cv2.imread('lana.jpg')\n",
    "resize = tf.image.resize(img1test, (256,256))\n",
    "yhat = model.predict(np.expand_dims(resize/255,0))\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
