{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from Cleaning import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "\n",
    "train = pd.read_csv('Raw Datasets/train.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train x from y and cleaning the dataset\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"].apply(lambda comment: clean(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and padding the dataset\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "tokenizer = Tokenizer(max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = tf.keras.preprocessing.sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the model\n",
    "\n",
    "# maxlen=100 as defined earlier\n",
    "inp = Input(shape=(100, ))\n",
    "\n",
    "# size of the vector space\n",
    "embed_size = 128\n",
    "x = Embedding(20000, embed_size)(inp)\n",
    "\n",
    "output_dimention = 60\n",
    "x = LSTM(output_dimention, return_sequences=True, name='lstm_layer')(x)\n",
    "# reduce dimention\n",
    "x = GlobalMaxPool1D()(x)\n",
    "# disable 10% precent of the nodes\n",
    "x = Dropout(0.1)(x)\n",
    "# pass output through a RELU function\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "# another 10% dropout\n",
    "x = Dropout(0.1)(x)\n",
    "# pass the output through a sigmoid layer, since \n",
    "# we are looking for a binary (0,1) classification \n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "# we use binary_crossentropy because of binary classification\n",
    "# optimise loss by Adam optimiser\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 128)          2560000   \n",
      "                                                                 \n",
      " lstm_layer (LSTM)           (None, 100, 60)           45360     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 60)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                3050      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 306       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,608,716\n",
      "Trainable params: 2,608,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "model.fit(X_t,y, batch_size=32, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('profanity_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82bd2656fced4debc06d60295b8d6c8b648b444f46985eddfe47fae284c5e8b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
